# Summary 🍃
#### *"🎶 End now, the end is near"*

### Scope of Work
- we abundantly explored & applied both classic ML and DL approaches for multiclass image classification
- we put our hands on myriad of feature extractors for classic ML approaches
- we built from scratch our own fine-tuned modifications of top-notch legacy (LeNet) models as well as contemporary TL (Google's MobileNetV2) models

### Challenges
- significantly lower hit rates on some of (in fact, just few & exclusively) underrepresented classes, despite a wide range of remedies applied
- facing homogeneous datasets (acquired in low-diversity setup), aiding and abetting overfitting & resisting generalization when untreated
- severe shortage of computational power at our disposal resulting in a few configurations tested and in everlasting training phases of our models

### Success Rate
- measured on validation/test images, our CNN models' accuracies hit high to **nearly perfect scores**, ranging from 85% (🥉 augmentation-aided LeNet-5), cracking 90% (🥈 LeNet) and **skyrocketing over 97%** (🥇 MobileNetV2)

### Can we do better than we do? <small>shout-out to Miley😉</small>
- hardly: via refined penalization
- likely: via augmentation
- highly likely: via segmentation
- almost surely: via ad hoc CNN architectures
- surely: via more data collection/acquisition
- undoubtedly: via employing XMSBoost (extreme monetary support boosting 🤣) to gear up for: 
    * broader hyperparameter tuning
    * (longer & faster) training

#### *"🎵 And so I face the final curtain..."*
> *Sinatra, Frank; Francois, C; Revaux; and Anka, Paul, "My Way" (1967)*
***